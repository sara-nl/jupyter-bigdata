{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting airplane delays using Random Forests ##\n",
    "\n",
    "In this exercise we are going to play a bit with a well known *Big Dataset* about plane trips. This notebook is an adaption of a scala notebook from the spark-notebook project:\n",
    "\n",
    "[https://github.com/andypetrella/spark-notebook](https://github.com/andypetrella/spark-notebook)\n",
    "\n",
    "We have translated the notebook to Python and adapted it for the jupyter platform.\n",
    "\n",
    "### The data ###\n",
    "\n",
    "For this small example we are going to use a subset of the [*Airline on-time performance*](http://stat-computing.org/dataexpo/2009/) data. For this exercise we will only use the data from the year 2008. This dat is already present in the workshop environment and we only have to load it into spark to get started. Let's explore what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize Spark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "if not 'sc' in globals():\n",
    "    conf = SparkConf().setMaster('local[*]')\n",
    "    sc = SparkContext(conf=conf)\n",
    "\n",
    "# authenticate for access to the HDFS\n",
    "!kinit.sh\n",
    "\n",
    "rawData = sc.textFile(\"2008.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(rawData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print type(rawData)\n",
    "header = rawData.first().split(\",\")\n",
    "print header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's strip the header and convert each line into an array. We will also make a randomsplit to reduce the amount of data for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = rawData.filter(lambda l: not(l.startswith(\"Year\"))).map(lambda l: l.split(\",\"))\n",
    "data, rest = data.randomSplit([0.001,0.999], 123456)\n",
    "print data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prettyprint a row with headers to see the kind of data we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "printSample = data.first()\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "th = [\"<th>\" + d + \"</th>\" for d in header]\n",
    "td = [\"<td>\" + d + \"</td>\" for d in printSample]\n",
    "\n",
    "display(HTML(\"<table><thead><tr>\" + \"\".join(th) + \"</tr></thead><tbody><tr>\" + \"\".join(td) + \"</tr></tbody></table>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of airports will probably come in handy. Let's make it. We'll simply use an array index (the airport names are in position 16 and 17) to get a list of distinct airports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "airportsRDD = data.filter(lambda a : (a[16] != \"NA\" and a[17] != \"NA\")).flatMap(lambda a : a[16:18]).distinct()\n",
    "print airportsRDD.count()\n",
    "print airportsRDD.take(10)\n",
    "airports = airportsRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More exploration\n",
    "\n",
    "While we now have a good idea about the structure of the data it's time to delve a little deeper. Let's examine the\n",
    "distributions of the various delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arrDelays = data.filter(lambda a : (a[14] != \"NA\")).flatMap(lambda a : ((str(a[16]), int(a[14])), (str(a[17]), int(a[14]))))\n",
    "depDelays = data.filter(lambda a : (a[15] != \"NA\")).flatMap(lambda a : ((str(a[16]), int(a[15])), (str(a[17]), int(a[15]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print arrDelays.take(4)\n",
    "print depDelays.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make histograms we will group delays by airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arrDelaysByAirportHist = arrDelays.groupByKey().map(lambda (x,y) : (x, list((y)))) \n",
    "depDelaysByAirportHist = depDelays.groupByKey().map(lambda (x,y) : (x, list((y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With the delays grouped we can plot some histograms for San Francisco. Note that this can take a while - actual evaluation of the RDD will take place here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arrDelaysH = arrDelaysByAirportHist.filter(lambda (x,y) : x == \"SFO\").map(lambda (x,y) : y).collect()\n",
    "depDelaysH = depDelaysByAirportHist.filter(lambda (x,y) : x == \"SFO\").map(lambda (x,y) : y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(arrDelaysH, bins=100, histtype='stepfilled', color=\"b\", label=\"arrival\")\n",
    "plt.hist(depDelaysH, bins=100, histtype='stepfilled', color=\"r\", alpha=0.5, label=\"departure\")\n",
    "plt.title(\"Delays for San Francisco airport: SFO\")\n",
    "plt.xlabel(\"Delay\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning: Random Forest\n",
    "\n",
    "Right, now let's apply some machine learning. First import some needed types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.linalg import DenseVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop some of the categorical features for simplicity. We will label the data with the departure delay and consider the following as features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = header[0:8] + header[11:15] + header[16:18] + header[18:21]\n",
    "print features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on to the boring (and slightly messy) part:\n",
    "* clean the data: if a row contains NA, drop it\n",
    "* convert the airport name to index in airport list for the feature vector\n",
    "* transform our data to the appropriate MLlib type: *LabeledPoint*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selectedData = data.filter(lambda a: \"NA\" not in a[0:21]).map(lambda a : \n",
    "                                   LabeledPoint(float(a[15]), \n",
    "                                   DenseVector([float(x) for x in a[0:8]] + \n",
    "                                               [float(x) for x in a[11:15]] + \n",
    "                                               [float(airports.index(a[16]))] + \n",
    "                                               [float(airports.index(a[17]))] + \n",
    "                                               [float(x) for x in a[18:21]]))).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual we split this data in training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training, testing = selectedData.randomSplit([0.7,0.3], 123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all this work we can finally train the model. Note again that actual evaluation of the training RDD will take place here and can take a few minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categoricalFeaturesInfo = {12:len(airports), 13:len(airports)}\n",
    "# For actual applications we would typically use much more trees\n",
    "numTrees = 10\n",
    "featureSubsetStrategy = \"auto\"\n",
    "impurity = \"variance\"\n",
    "maxDepth = 4\n",
    "maxBins = len(airports)\n",
    "\n",
    "model = RandomForest.trainRegressor(training, categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although they are hard to interpret we can take a look at the trained trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Learned regression forest model:\\n\" + model.toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to predict some test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = testing.take(10)\n",
    "predictions = [(point.label, model.predict(point.features), point.label - model.predict(point.features)) for point in test]\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "tbody = \"\"\n",
    "for tup in predictions:\n",
    "    tbody = tbody + \"<tr><td>\" + str(tup[0]) + \"</td><td>\" + str(tup[1]) + \"</td><td>\" + str(tup[2]) + \"</td></tr>\"\n",
    "display(HTML(\"<table><thead><tr><th>Actual delay</th><th>Predicted delay</th><th>Difference</th></tr></thead><tbody><tr>\" + tbody + \"</tr></tbody></table>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the MSE. First a baseline and then our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get some baseline\n",
    "avgDelay = training.map(lambda p: p.label).mean()\n",
    "print avgDelay\n",
    "baseLineData = testing.map(lambda p: (p.label, avgDelay))\n",
    "print baseLineData.first()\n",
    "mseBaseLine = baseLineData.map(lambda (x,y) : (x - y)**2).mean()\n",
    "print mseBaseLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(testing.map(lambda p: p.features))\n",
    "labelsAndPredictions = testing.map(lambda p: p.label).zip(predictions)\n",
    "print labelsAndPredictions.first()\n",
    "testMSE = labelsAndPredictions.map(lambda (x,y) : (x - y)**2).mean()\n",
    "print testMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This concludes the notebook. Feel free to experiment more with the model and the data. As an exercise try plotting the predicted and actual values. Alternatively you can try to find out if there are certain airports were the predictions are significantly better. Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
