{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left\" src=\"images/spark.png\" />\n",
    "<img style=\"float: right\" src=\"images/surfsara.png\" />\n",
    "<hr style=\"clear: both\" />\n",
    "\n",
    "# Introduction to Apache Spark\n",
    "\n",
    "In this final notebook we will explore Spark using the Python API, called PySpark.\n",
    "\n",
    "_You can edit the cells below and execute the code by selecting the cell and press Shift-Enter. Code completion is supported by use of the Tab key._\n",
    "\n",
    "During the exercises you may want to refer to the [PySpark documentation](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html#pyspark.RDD) for more information on possible transformations and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize Spark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "if not 'sc' in globals():\n",
    "    conf = SparkConf().setMaster('local[*]')\n",
    "    sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a RDD from a list\n",
    "\n",
    "All parallel work in Spark is done on RDDs, so the first thing we need to do is convert our data (in this case a list) to an RDD. We will use the `parallelize` method on the `SparkContext sc`. It takes two arguments: (1) the collection, and (2) the number of partitions (splits). The second argument is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_list = ['Dog', 'Cat', 'Rabbit', 'Hare', 'Deer', 'Gull', 'Woodpecker', 'Mole']\n",
    "words_rdd = sc.parallelize(words_list, 2)\n",
    "\n",
    "print 'the type of words_list is: ' + str(type(words_list))\n",
    "print 'the type of words_rdd is: ' + str(type(words_rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map transformation\n",
    "\n",
    "We now want to change all words in the `words_rdd` to their plural form. We will do this using a [map](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=map#pyspark.RDD.map) transformation.\n",
    "Remember that the map function will apply the function to each element of the RDD. \n",
    "\n",
    "First, we will write a simple function that takes a single word as argument and return the word with an 's' added to it. In the next step we will use this function in a map transformation of the `words_rdd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def make_plural(word):\n",
    "    \"\"\"Adds an 's' to `word`.\n",
    "\n",
    "    Note:\n",
    "        This is a simple function that only adds an 's'.  \n",
    "\n",
    "    Args:\n",
    "        word (str): A string.\n",
    "\n",
    "    Returns:\n",
    "        str: A string with 's' added to it.\n",
    "    \"\"\"\n",
    "    return <FILL IN>\n",
    "\n",
    "print make_plural('cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the `makePlural` function as input for the map transformation on `words_rdd`.\n",
    "The action [`collect`](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=collect#pyspark.RDD.collect) transfers the content of the RDD to the driver. Note, that a large RDD may be scattered over many machines. In such a case a `collect` can be a very bad idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "plural_rdd = words_rdd.map<FILL IN>\n",
    "print plural_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using lambda functions\n",
    "\n",
    "But wait a minute! We can actually achieve the same functionality by using lambda functions. In this case we define makePlural as a lambda function. In this case we define `makePlural` as a lambda function. \n",
    "\n",
    "Hint: The map function needs a function as argument. This function needs one argument, let's call that `x`. The body of the function adds an 's' to the end of `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# A lambda function for adding s at the end of a string\n",
    "lambda_plural_rdd = words_rdd.map(<FILL IN>)\n",
    "\n",
    "print lambda_plural_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do another map transformation. For each word in `words_rdd` determine its length. The Python function `len`  will return the length of a string.\n",
    "\n",
    "You can do this with a lambda function, but there is another way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "word_lengths = (<FILL IN>\n",
    "                 .collect())\n",
    "print word_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your solution by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(sorted(word_lengths), [3, 3, 4, 4, 4, 4, 6, 10], 'incorrect value for word_lengths')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## RDD from a text file\n",
    " \n",
    "Manipulating a list with eight elements gets boring pretty fast, so let's move start processing a file. In this part we will read the file `alice.txt` from HDFS and will count the number of occurences of every word. This 'Word Count' is the ['Hello World'](https://en.wikipedia.org/wiki/%22Hello,_World!%22_program) of data-processing frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# authenticate for access to the HDFS\n",
    "!kinit.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create an RDD from alice.txt where every element is a line of the file\n",
    "alice_rdd = sc.textFile('alice.txt').cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `collect` call not accepted\n",
    "\n",
    "As we mentioned before, once your data no longer fits on the screen the `collect` method becomes less useful or even problematic. But we still want to have a way to inspect the (intermediate) results. For this we can use one of the following methods as a replacement of `collect`:\n",
    "\n",
    "- `first()`: return the first elements of the rdd \n",
    "- `take(n)`:  return a list of `n` elements\n",
    "- `takeOrdered(n, [key=f])`: return the first n elements of the rdd, the order is defined by the optional function f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "print 'first: ' + alice_rdd.first()\n",
    "print 'take(5): '\n",
    "pp.pprint(alice_rdd.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The one-to-many map: flatMap\n",
    "\n",
    "We have an RDD of lines, so let's try to convert this to an RDD of words by splitting the lines on whitespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alice_words_try = alice_rdd.map(lambda line: line.split())\n",
    "\n",
    "pp.pprint(alice_words_try.take(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look right! We want an RDD of words, but we have created an RDD of lists of words.\n",
    "\n",
    "We want to map a function on an input that returns multiple values in a list, but then not to want the output nested in the same way as the input was. As it is commonly needed Spark includes a [`flatMap`](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.flatMap) transformation that will _flatten_ the output of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alice_words = alice_rdd.flatMap(lambda line: line.split())\n",
    "\n",
    "pp.pprint(alice_words.take(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key-Value Pairs\n",
    "\n",
    "In order to count words in parallel we are going to use an RDD which consist of simple key-value pairs. We will call this RDD `alice_pairs` and it will be result of a transformation of `alice_words`. For every word in `alice_words` we want to have a `(word, 1)` tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "alice_pairs = <FILL IN>\n",
    "\n",
    "print alice_pairs.takeOrdered(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Test.assertEquals(alice_pairs.takeOrdered(10),\n",
    "                  [(u'\"\\'--found', 1), (u'\"\\'Tis', 1), (u'\"--\\'And', 1), (u'\"A', 1), (u'\"A', 1), (u'\"Ah,', 1), (u'\"Ahem!\"', 1), (u'\"Alice!\"', 1), (u'\"And', 1), (u'\"And', 1)],\n",
    "                  'incorrect value for alice_pairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReduceByKey\n",
    "\n",
    "Next, we are going to count all words by using [`reduceByKey`](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=reducebykey#pyspark.RDD.reduceByKey).\n",
    "\n",
    "`ReducebyKey` expects the RDD to consist of key-value pairs an it will perform a reduce operation per key. \n",
    "It will need a two-argument function as input that will work on the values only. Remember that a reduce function needs two arguments and will reduce all elements to a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Note that reduceByKey takes in a function that accepts two values and returns a single value\n",
    "# The function that is input to reduceByKey only works on the values. Spark will execute this function per key\n",
    "\n",
    "word_counts = alice_pairs.reduceByKey(lambda x,y : <FILL IN>)\n",
    "\n",
    "top10_words = word_counts.takeOrdered(10, key=lambda p: -p[1])\n",
    "\n",
    "print top10_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST Counting using reduceByKey\n",
    "Test.assertEquals(top10_words,\n",
    "                  [(u'the', 577), (u'and', 296), (u'a', 265), (u'to', 241), (u'she', 197), (u'of', 197), (u'was', 159), (u'in', 157), (u'said', 129), (u'it', 114)]\n",
    ",\n",
    "                  'incorrect value for word_counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReduceByKey - background\n",
    "\n",
    "The `reduceByKey` method is similar to Hadoop's Reduce, but more restrictive. The function you provide to `reduceByKey` needs to be both [commutative](https://en.wikipedia.org/wiki/Commutative_property) and [associative](https://en.wikipedia.org/wiki/Associative_Property).\n",
    "\n",
    "These restrictions allow Spark to perform additional optimisations, performing the operation on each partitions of the RDD and minimising network traffic. In the example below at most six values need to be transmitted.\n",
    "\n",
    "![reduceByKey](images/reducebykey.png)\n",
    "(Picture by DataBricks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
